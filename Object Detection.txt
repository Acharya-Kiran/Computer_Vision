----------------------------------------------------------------------------------------------------
Template Matching :
----------------------------------------------------------------------------------------------------
- Simplest form of object detection, i.e. it simply looks for an exact copy of an image in another image.
- It scans the larger image for a provided template by sliding the template target image across the larger image.
- the param that can be adjusted is the comparision method which is some sort of correlation based metric.
          -------#  measure whether or not there is a relationship between two variables
Restriction : the matching template should have the same size for the portion it is searching for in the larger image.

ex. mysting = 'sum'
    myfunc = eval(mystring)
    myfunc([1,2,3]) ---> 6
# eval func is used when you want to try different methods on an object.

#comaparision methods
methods = ['cv2.TM_CCOEFF','cv2.TM_CCOEFF_NORMED',...]
for m in methods:
    img_copy = img.copy()
    method = eval(m)
    res = cv2.matchTemplate(img_copy,template,method)    #--------- gives a heat map
    min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res) #--------- then you find max & min values and their locations from the heat map and use it to draw rectangles
    if method in [cv2.TM_SQDIFF,TM_SQDIFF_NORMED]: #These methods actually work on the minimium. i.e. darker portion of heat map is the matched portion.
	top_left = min_loc #(x,y)
    else:
	top_left = max_loc
    height, width, channels = template.shape
    bottom_right = (top_left[0]+width,top_left[1]+height)
    cv2.rectangle(img_copy,top_left,bottom_right,(255,0,0),10)

----------------------------------------------------------------------------------------------------
Corner Detection:
----------------------------------------------------------------------------------------------------
- Looking for corners in images
- Corner is a point whose local neighborhood stands in two dominant & different edge directions OR junction of two edges where, edge is sudden change in image brightness.
Algo's:
1. Harris Corner Detection
intution - Corners can be detected by looking for significant change in all directions.
f(x,y) = SUMMATION(xk,yk)EW (I(xk,yk)-I(xk+delta(x),yk+delta(y)))^2
#we need floting values in img matrix for corner-harris, Shi-Tomasi detection algo.
 img = cv2.imread("path");
 gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
 gray_ = np.float32(gray)
 dst = cv2.cornerHarris(src=gray_,blockSize=2,ksize=3,k=0.04) #blocksize - neighborhood size & thats how it actually detects corner eigenvalues & vectors,
                                                              #ksize -  aperture/kernel size param for "sobel" ------------------------------------
              						      #k - harris detector free param
 #the result is dilated for marking the corners.
 dst = cv2.dilate(dst,None)


2. Shi-Tomasi Corner Detection - made a small modification to Harris-Corner which ended up with better results.
It changes the scoring function selection criteria that Harris uses for corner detection.
Harris uses : R = lambda1*lambda2 - k(lambda1+lambda2) #####--------------------- k : harris detector free param, value in range [0.04,0.06] : (lets tradeoff between
                                                      (precision & recall, high value - less false corners, low-value - more corners ) 
Shi-Tomasi : R = min(lambda1,lambda2)

-corners = cv2.goodFeaturesToTrack(gray_,max_num_of_corners_req/-1 to return all corners,quality_level_param(multiplied with min eigen value or R),min_dist)

----------------------------------------------------------------------------------------------------
Edge detection:
----------------------------------------------------------------------------------------------------
Expanding to find general edges of objects.
- Canny Edge Detector process: 
. Apply Gaussian/Custom Blur to remove noise and hence smooth the image.
. Find the intensity gradients of the image : Smoothened image is then filtered with a 'Sobel' kernel in both horizontal and vertical direction to get first derivative
  in horizontal direction (G_x) and vertical direction (G_y). 
. Apply non-maximum suppression to get rid of spurious response to edge detection : After getting gradient magnitude and direction, a full scan of image is done to remove any
   unwanted pixels which may not constitute the edge. For this, at every pixel, pixel is checked if it is a local maximum in its neighborhood 
   in the direction of gradient.  If so, it is considered for next stage, otherwise, it is suppressed ( put to zero). the result you get is a binary image with “thin edges”.
. Apply double threshold to determine potential edges	
. Track edge by hysteresis - find edges by suppressing all other those are weak & not connected to strong edges.
the algo requires user to decide on low and high threshold vals.

edges = cv2.Canny(img,threshold1,threshold2)
	

----------------------------------------------------------------------------------------------------
Grid Detection:
----------------------------------------------------------------------------------------------------
- Combining Corner & Edge Detection to find grid like patterns in images.
- Often Cameras can cause distortion in an image such as radial and tangential distortion. A good way to account for these distortions when performing operations like
 object tracking is to have a recognizable pattern attached to the object being tracked.
- Grid pattterns are often used to calibrate cameras and track motion.
- The grid should be specifically made to look like some sort of checkerboard / chess board type image and then placed on whatever you are trying to calibrate your
 camera with.
ex1.
 flat_chess = cv2.imread("path")
-- found,corners = cv2.findChessboardCorners(flat_chess,(7,7)) #grid_size : we exclude the border corners of a chess board as there is no neighborhood to find gradient
 #found - boolean indicating whether a chess board was found or not.
 #corners - co-ordinates in image where its found.	
-- cv2.drawChessboardCorners(flat_chess,(7,7),corners,found) #---corners in every row is plotted in seperate color and lines are drawn across each corner in row pattern 
                                                             where the last corner is connected to first corner in next row via a line
 plt.imshow(flat_chess)

ex2.
 dots = cv2.imread("path")
-- found,corners = cv2.findCirclesGrid(dots,(10,10),cv2.CALIB_CB_SYMMETRIC_GRID) #is the way it actually searches for it.
-- cv2.drawChessboardCorners(dots,(10,10),corners,found)


----------------------------------------------------------------------------------------------------
Contour Detection:
----------------------------------------------------------------------------------------------------
Allows to detect foreground vs background images. also allows for detection of external vs internal contours.(ex. grabbing eyes & smile from a face)
- Contours are defined as simply a curve that joining all continuous points (along the boundary), having same color or intensity.
- Contours are useful tool for shape analysis, object detection and recognition.
- For better accuracy, use binary images. So before finding contours, apply threshold or canny edge detection.
- In OpenCV, finding contours is like finding white object from black background. So, object to be found should be white and background should be black.

image,contours,hierarchy = cv2.findContours(img,cv2.RETR_CCOMP (to recieve both internal & external contours/ Contour retrieval mode),cv2.CHAIN_APPROX_SIMPLE (Contour approximation method))
#----Contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. 
#----But does it store all the coordinates ? That is specified by this contour approximation method.
#----cv2.CHAIN_APPROX_SIMPLE removes all redundant points and compresses the contour, thereby saving memory.
#hierarchy is a np_array which defines the pointss where contours are and has a value -ve/+ve indicating external or internal contour respectively.

external_contours = np.zeros(img.shape) #background to display contours on
for i in range(len(contours)):
    if hierarchy[0][i][3] == -1: #------external contours - are the things that are directly touching what it actually believes to be the background
	cv2.drawContours(external_contours,contours,i,255 (color to display),-1) #-1 indicates its filled
    else:    #-----------internal contours are grouped differently using the value in hierarchy
	cv2.drawContours(external_contours,contours,i,255 (color to display),-1)
plt.imshow(external_contours,cmap='gray')


----------------------------------------------------------------------------------------------------
Feature Matching:
----------------------------------------------------------------------------------------------------
It extracts defining key features from an input image (using ideas such as corner,edge,contour detection), Then using distance calculation it finds all the matches in
a secondary image.

Advanced method of detecting matching objects in another image.	
##----Brute-Force matcher is simple. It takes the descriptor of one feature in first set and is matched with all other features in second set 
##----using some distance calculation. And the closest one is returned.

----What is this Matcher Object?
----The result of matches = bf.match(des1,des2) line is a list of DMatch objects. This DMatch object has following attributes:

----DMatch.distance - Distance between descriptors. The lower, the better it is.
----DMatch.trainIdx - Index of the descriptor in train descriptors
----DMatch.queryIdx - Index of the descriptor in query descriptors
----DMatch.imgIdx - Index of the train image.

1- Brute-Force Matching with ORB(Oriented fast & Rotated Brief) descriptors #------Binary string based descriptors, uses Hamming distance as measurement(cv2.NORM_HAMMING)
orb = cv2.ORB_create() #---create a detector
kp1,des1 = orb.detectAndCompute(target_img,None(option for mask)) #---returns key points & descriptors
kp2,des2 = orb.detectAndCompute(secondary_img,None(option for mask))
bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=True) #---create BFMatcher object
 #1st param - norm type, indicates the distance measurement to be used.
 #2nd param is boolean variable, crossCheck which is false by default. If it is true, Matcher returns only those matches with 
 value (i,j) such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa.
matches = bf.match(des1,des2)
matches = sorted(matches,key=lambda x:x.distance)
#-----distance specifies how good of a match it was. less dist->better match, more dist->worse match
res = cv2.drawMatches(target_img,kp1,secondary_img,kp2,matches[:25],None(Mask option),flags=2)


2- Brute-Force Matching with SIFT(Scale Invariant Feature Transform) descriptors & Ratio Test //uses KNN
sift = cv2.xfeatures2d.SIFT_create() #---create a detector
kp1,des1 = sift.detectAndCompute(target_img,None) #---returns key points & descriptors
kp2,des2 = sift.detectAndCompute(secondary_img,None)
bf = cv2.BFMatcher() #default params are - cv2.NORM_L2, crossCheck = False
matches = bf.knnMatch(des1,des2,k=2) #--------finds k best matches for each descriptor from a query set
###----------Ratio test is to check whether k paired list of matches are relatively close to each other in distance or not.
good = []
for match1,match2 in matches:
    if match1.distance < 0.75*match2.distance :   #----Ratio Test, Less distance- Better match
	good.append([match1])
res = cv2.drawMatchesKnn(target_img,kp1,secondary_img,kp2,good,None,flags=2)

Note : KNN -->


3- FLANN(Fast Library for Aproximate Nearest Neighour) based matcher //uses KDTree
It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and 
for high dimensional features. It works more faster than BFMatcher for large datasets.
For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc. First one is IndexParams.
Second dictionary is the SearchParams. It specifies the number of times the trees in the index should be recursively traversed. Higher values gives better precision,
but also takes more time.
##----FLANN is much faster than Brute Force matches but it only finds Approximate nearest neighbors which is good matching but not necessarily the best.
sift = cv2.xfeatures2d.SIFT_create() #---create a detector
kp1,des1 = sift.detectAndCompute(target_img,None) #---returns key points & descriptors
kp2,des2 = sift.detectAndCompute(secondary_img,None)
#---FLANN params
FLANN_INDEX_KDTREE = 0
index_params = dict(algorithms=FLANN_INDEX_KDTREE ,trees=5)
search_params = dict(checks=5)
flann = cv2.FlannBasedMatcher(index_params,search_params)
matches = flann.knnMatch(des1,des2,k=2)
good = []
for match1,match2 in matches:
    if match1.distance < 0.75*match2.distance :   #----Ratio Test, Less distance- Better match
	good.append([match1])
res = cv2.drawMatchesKnn(target_img,kp1,secondary_img,kp2,good,None,flags=2)

Note: KDTree --> 



----------------------------------------------------------------------------------------------------
Watershed Algorithm:
----------------------------------------------------------------------------------------------------
- Advanced algo that alows us to segment images into foreground & background. allows us to manually set seeds to choose segments of an image.
- Watersheds can be segmented as topographical maps with boundaries.
- Any gray scale image can be viewed as a topological surface where high intensity denotes peaks and hills while low intensity denotes valleys.
- The algo can then fill every isloated valleys(local minima) with different colored water(labels).
- As the water rises depending on the peaks nearby(gradients), water from different valleys (different segments) with different colors could start to merge.
- To aoid this merging the algo creates barriers(segment edge boundaries) in location where water merges.
ex.
img = cv2.imread('path')
img = cv2.medianBlur(img,35) #square kernel size
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
ret,thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY_INV)	
#---optional noise removal
kernel = np.ones((3,3),np.uint8)
opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel,itertions=2)
sure_bg = cv2.dilate(opening,kernel,iterations=3	)
----
##----- Watershed algorithm requires to set seeds that we are sure in the foreground.
##------- To get this what we can do is a ****distance transform.****: you should have a binary image with 0&1's or 0&255's.
##------- what it does is, it increases the values of the pixels that are further away from 0's. (0 being the background)
##------- then we would expect the every centres of the shapes to be bright and slowly fade away into dark gray as you go away from the centre.
##-------Then you apply threshold and get the fixed foreground points for those shapes.

dist_transform = cv2.distanceTransform(opening,cv2,DIST_L2,5) # distance_type, mark_size
ret,sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)
 #---sure_fg with have the centre points of the shapes and the rest part of the shape has to be identified by the Watershed Algorithm.
sure_fg = np.uint8(sure_fg)
unknown = cv2.subtract(sure_bg,sure_fg) #the regions that we are unsure of being fg or bg

#-----You then need to make 'label markers' out of those sure fg points and have those be the seeds that the watershed algo uses to find its segments.
ret,markers = cv2.connectedComponents(sure_fg) #this will color each marker differently 
markers = markers + 1 #you gray the background
markers[unknown==255] = 0 #mark the unknown region as black
markers = cv2.watershed(img,markers) #you get distinct segments
#----then find contours and map the boundaries on the original image.
image,contours,hierarchy = cv2.findContours(makers.copy(),cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE)
for i in range(len(contours)):
    if hierarchy[0][i][3] == -1:
	cv2.drawContours(img,contours,i,(255,0,0),10) #10-line thickness

----------------------------------------------------------------------------------------------------
Watershed Algorithm with Custom Seeds
----------------------------------------------------------------------------------------------------
Custom seeds to the algo can be provided with the help of mouse-click event.
ex.

road = cv2.imread('path')
road_copy = np.copy(road) OR road.copy()
marker_image = np.zeros(road.shape[:-2],dtype=np.int32)
segments = np.zeros(road.shape,dtype=uint8)

from matplotlib import cm #colormappings
cm.tab10(0) #returns (R,G,B,aplha) value. here alpha is 1.0 indicating values of intensities are in range 0-1. So we multiplyt it by 255 	

def create_rgb(i):
    return tuple(np.array(cm.tab10(i)[:3])*255)
colors = []
for i in range(10):
    colors.append(create_rgb(i))
##
#-Global variables
n_markers = 	10
current_marker = 1 #-----color choice
marker_updated = False #--- markers updated by Watershed

#-Callback 
def mouse_callback(event,x,y,flags,param):
    global marks_updated
    if event == cv2.EVENT_LBUTTONDOWN:
	#MARKER PASSED TO WATERSHED ALGO
	cv2.circle(marker_image,(x,y),10#radius,(current_marker),-1)
	#USER SEES ON ROAD IMG
	cv2.circle(road_copy,(x,y),10#radius,colors[current_marker],-1)
	marker_updated = True
cv2.namedWindow('Road Image')
cv2.setMouseCallback('Road Image',mouse_callback)
while True:
    cv2.imshow('Watershed segments',segments) #segments would initially be a black image
    cv2.imshow('Road Image',road_copy)
    #CLOSE ALL WINDOWS
    k = cv2.waitKey(1) & 0xFF
    if k == 27:
	break
    #CLEARING ALL COLORS
    elif k == ord('c'):
	road_copy = np.copy(road) OR road.copy()
	marker_image = np.zeros(road.shape[:-2],dtype=np.int32)
	segments = np.zeros(road.shape,dtype=uint8)
    #UPDATE COLOR CHOICE
    elif k>0 and char(k).isDigit():
	current_marker = int(char(k))

    #UPDATE MARKINGS
    if marks_updated:
	marker_image_copy = marker_image.copy()
	cv2.watershed(road,marker_image_copy)
	segments = np.zeros(road.shape,dtype=uint8)

	for color_ind in range(n_markers):
	    segments[marker_image_copy==(color_ind)] = colors[color_ind]

cv2.destroyAllWindows()
  	
##---so here you have 2 images displayed side by side. you select a color, click on 1st image, the color and location of point will be used as seed and segment
##---would be displayed with the same color on 2nd image.


----------------------------------------------------------------------------------------------------
Facial & eye detection:
----------------------------------------------------------------------------------------------------
use Viola-Jones algorithm with Haar-Cascades to detect faces in images, based on a simple concept of few key features. They also came up with the idea of pre-computing
an integral image to save time on calculations.
The main features are,
- Edge features
- Line features
- Four-Rectangle features
Each feature is a single value obtained by subtracting sum of pixels under white rectangle from sum under black rectangle. The closure the feature is to 1, better the
feature. Calculating these sums for the entire image would be very Computationally expensive. The algorithm solves this by using the integral image, resulting in an
O(1) running time of the algorithm.
-- An Integral image is also known as a summed area table. ex. It helps us calculate the sum of any rectangle/ sub-rectangles in actual image. comes handy when you want to subract 
   light region from a dark region.
-- The Algorithm also saves time by going through a cascade of classifiers, which means the image is treated with the series of classifiers based on simple features.
-- Once the image fails a classifier, the attempt to detect a face can be stopped.
##-- Downside of this algo is the very large dataset needed to create your own features. Luckily many pre-trained sets of features exist.		
##-- OpenCV comes with pre-trained xml files of various Haar-Cascades.

ex. 
#----the operations are performed on a grayscale image
#-------The Haar Cascades are able to detect the face for upto 45 degree of variation.

import cv2
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline                      #--- not necessary for latest jupyter notebook versions

nadia = cv2.imread('path',0) #---image with a single face. 0 - indicates the extraction of grayscale/single-channel image from colored one.
solvay = cv2.imread('path',0) #--group photo.

face_cascade = cv2.CascadeClassifier('../haarcascade_frontalface_default.xml') #--this classifier contains over 6000 features. there are multiple of the same kind.

def detect_face(img):
    face_img = img.copy()
    face_rects = face_cascade.detectMultiScale(face_img) #--returns x,y of top left corner along with width,height of rectangle consisting the match of features.
    for (x,y,w,h) in face_rects:
        cv2.rectangle(face_img,(x,y),(x+w,y+h),(255,255,255),10) #--last 2 features are color & thickness
    return face_img
result = detect_face(nadia)
plt.imshow(result,cmap='gray')

#---when the same method is applied to a photo of multiple faces, it might detect noise or have multiple rectangles on same face. So we add few parameters specifically
#--- scale factor      - param specifying how much the image size is reducing at each image scale
#--- minimum_neighbors - param specifying how many neighbors each candidate rectangle should have to retain it.

def adj_detect_face(img):
    face_img = img.copy()
    face_rects = face_cascade.detectMultiScale(face_img, scaleFactor=1.2, minNeighbors=5) #default values
    for (x,y,w,h) in face_rects:
        cv2.rectangle(face_img,(x,y),(x+w,y+h),(255,255,255),10) #--last 2 features are color & thickness
    return face_img
result = adj_detect_face(solvay)
plt.imshow(result,cmap='gray')

eye_cascade = cv2.CascadeClassifier('../haarcascade_eye.xml')  
#---the features are able to detect eyes assuming they are brighter than almost all parts of face, and may not work with edited photos.

def detect_eyes(img):
    face_img = img.copy()
    eyes_rects = eye_cascade.detectMultiScale(face_img, scaleFactor=1.2, minNeighbors=5) 
    for (x,y,w,h) in eyes_rects:
        cv2.rectangle(face_img,(x,y),(x+w,y+h),(255,255,255),10) #--last 2 features are color & thickness
    return face_img
result = detect_eyes(nadia)
plt.imshow(result,cmap='gray')

#--detect faces in a video
cap = cv2.VideoCapture(0)
while True:
    ret,frame = cap.read(0)
    frame = detect_frame(frame)
    cv2.imshow('Video Face Detect',frame)
    k = cv2.waitKey(1)
    if k==27:
	break
cap.release()
cv2.destroyAllWindows()

Note : We need a large dataset and Deep Learning for Facial Recognition.

